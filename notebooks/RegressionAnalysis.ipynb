{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Regression Analysis for the California Housing census</h1>\n",
    "\n",
    "<p style=\"text-align:center\">Author: Jose Pena</p>\n",
    "<p style=\"text-align:center\">Github: <a href=\"https://github.com/JoseJuan98\">JoseJuan98</a></p>\n",
    "<br>\n",
    "\n",
    "------------------------------------------------------------------------------------------------\n",
    "\n",
    "<br>\n",
    "\n",
    "## Aims\n",
    "\n",
    "The aim of this notebook is to demonstrate the data regression skills leveraging a wide variety of tools, but also this report focuses on present findings, insights, and next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "This exercise will demonstrate the data regression skills. You are expected to leverage a wide variety of tools, but also this report should focus on present findings, insights, and next steps. You may include some visuals from your code output, but this report is intended as a summary of your findings, not as a code review.\n",
    "\n",
    "The development will center around 5 main points:\n",
    "\n",
    "1.  Does the report include a section describing the data?\n",
    "2.  Does the report include a paragraph detailing the main objective(s) of this analysis?\n",
    "3.  Does the report include a section with variations of linear regression models and specifies which one is the model that best suits the main objective(s) of this analysis.\n",
    "4.  Does the report include a clear and well-presented section with key findings related to the main objective(s) of the analysis?\n",
    "5.  Does the report highlight possible flaws in the model and a plan of action to revisit this analysis with additional data or different predictive modeling techniques?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have selected a data set, you will produce the deliverables listed below and submit them to one of your peers for review. Treat this exercise as an opportunity to produce analysis that are ready to highlight your analytical skills for a senior audience, for example, the Chief Data Officer, or the Head of Analytics at your company.\n",
    "Sections required in your report:\n",
    "\n",
    "*   Main objective of the analysis that specifies whether your model will be focused on prediction or interpretation.\n",
    "*   Brief description of the data set you chose and a summary of its attributes.\n",
    "*   Brief summary of data exploration and actions taken for data cleaning and feature engineering.\n",
    "*   Summary of training at least three linear regression models which should be variations that cover using a simple  linear regression as a baseline, adding polynomial effects, and using a regularization regression. Preferably, all use the same training and test splits, or the same cross-validation method.\n",
    "*   A paragraph explaining which of your regressions you recommend as a final model that best fits your needs in terms of accuracy and explainability.\n",
    "*   Summary Key Findings and Insights, which walks your reader through the main drivers of your model and insights from your data derived from your linear regression model.\n",
    "*   Suggestions for next steps in analyzing this data, which may include suggesting revisiting this model adding specific data features to achieve a better explanation or a better prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "-------------------------------------------"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup\n",
    "\n",
    "For this Regression Data Analysis project the following libraries will be used:\n",
    "\n",
    "*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2021-01-01) for managing the data.\n",
    "*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2021-01-01) for mathematical operations.\n",
    "*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2021-01-01) for visualizing the data.\n",
    "*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2021-01-01) for visualizing the data.\n",
    "*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2021-01-01) for machine learning related functions.\n",
    "\n",
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, ElasticNetCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from src.utils.data_transformations import preprocess_data\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# utils.py\n",
    "from src.utils import fetch_housing_data, HOUSING_URL, HOUSING_PATH, TARGET"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setting up some options:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Display and store plot within the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Show all columns when displaying dataframe\n",
    "pandas.options.display.max_columns = None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " Load data, if it wasn't downloaded before it will fetch it from the URL specified, otherwise will load it from a local '.csv' file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data = fetch_housing_data(url=HOUSING_URL,\n",
    "                          path=HOUSING_PATH)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. About the Data\n",
    "\n",
    "This California Housing dataset is available from [Lu√≠s Torgo's page](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html) (University of Porto).\n",
    "\n",
    "This dataset appeared in a 1997 paper titled Sparse Spatial Autoregressions by Pace, R. Kelley and Ronald Barry, published in the Statistics and Probability Letters journal. They built it using the 1990 California census data. It contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).\n",
    "\n",
    "The target variable or dependent variable for this anlysis will be the `median_house_value`, which describes median price of the houses per block group.\n",
    "\n",
    "**Shape of the dataset**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(20640, 10)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**List of columns**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "['longitude',\n 'latitude',\n 'housing_median_age',\n 'total_rooms',\n 'total_bedrooms',\n 'population',\n 'households',\n 'median_income',\n 'median_house_value',\n 'ocean_proximity']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns.to_list()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**First 5 rows of the dataset**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n0    -122.23     37.88                41.0        880.0           129.0   \n1    -122.22     37.86                21.0       7099.0          1106.0   \n2    -122.24     37.85                52.0       1467.0           190.0   \n3    -122.25     37.85                52.0       1274.0           235.0   \n4    -122.25     37.85                52.0       1627.0           280.0   \n\n   population  households  median_income  median_house_value ocean_proximity  \n0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n4       565.0       259.0         3.8462            342200.0        NEAR BAY  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n      <th>median_house_value</th>\n      <th>ocean_proximity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-122.23</td>\n      <td>37.88</td>\n      <td>41.0</td>\n      <td>880.0</td>\n      <td>129.0</td>\n      <td>322.0</td>\n      <td>126.0</td>\n      <td>8.3252</td>\n      <td>452600.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-122.22</td>\n      <td>37.86</td>\n      <td>21.0</td>\n      <td>7099.0</td>\n      <td>1106.0</td>\n      <td>2401.0</td>\n      <td>1138.0</td>\n      <td>8.3014</td>\n      <td>358500.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-122.24</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1467.0</td>\n      <td>190.0</td>\n      <td>496.0</td>\n      <td>177.0</td>\n      <td>7.2574</td>\n      <td>352100.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1274.0</td>\n      <td>235.0</td>\n      <td>558.0</td>\n      <td>219.0</td>\n      <td>5.6431</td>\n      <td>341300.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1627.0</td>\n      <td>280.0</td>\n      <td>565.0</td>\n      <td>259.0</td>\n      <td>3.8462</td>\n      <td>342200.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Non-null values count, type of feature and memory usage**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   longitude           20640 non-null  float64\n",
      " 1   latitude            20640 non-null  float64\n",
      " 2   housing_median_age  20640 non-null  float64\n",
      " 3   total_rooms         20640 non-null  float64\n",
      " 4   total_bedrooms      20433 non-null  float64\n",
      " 5   population          20640 non-null  float64\n",
      " 6   households          20640 non-null  float64\n",
      " 7   median_income       20640 non-null  float64\n",
      " 8   median_house_value  20640 non-null  float64\n",
      " 9   ocean_proximity     20640 non-null  object \n",
      "dtypes: float64(9), object(1)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Statistical properties of the dataset**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "           longitude      latitude  housing_median_age   total_rooms  \\\ncount   20640.000000  20640.000000        20640.000000  20640.000000   \nunique           NaN           NaN                 NaN           NaN   \ntop              NaN           NaN                 NaN           NaN   \nfreq             NaN           NaN                 NaN           NaN   \nmean     -119.569704     35.631861           28.639486   2635.763081   \nstd         2.003532      2.135952           12.585558   2181.615252   \nmin      -124.350000     32.540000            1.000000      2.000000   \n25%      -121.800000     33.930000           18.000000   1447.750000   \n50%      -118.490000     34.260000           29.000000   2127.000000   \n75%      -118.010000     37.710000           37.000000   3148.000000   \nmax      -114.310000     41.950000           52.000000  39320.000000   \n\n        total_bedrooms    population    households  median_income  \\\ncount     20433.000000  20640.000000  20640.000000   20640.000000   \nunique             NaN           NaN           NaN            NaN   \ntop                NaN           NaN           NaN            NaN   \nfreq               NaN           NaN           NaN            NaN   \nmean        537.870553   1425.476744    499.539680       3.870671   \nstd         421.385070   1132.462122    382.329753       1.899822   \nmin           1.000000      3.000000      1.000000       0.499900   \n25%         296.000000    787.000000    280.000000       2.563400   \n50%         435.000000   1166.000000    409.000000       3.534800   \n75%         647.000000   1725.000000    605.000000       4.743250   \nmax        6445.000000  35682.000000   6082.000000      15.000100   \n\n        median_house_value ocean_proximity  \ncount         20640.000000           20640  \nunique                 NaN               5  \ntop                    NaN       <1H OCEAN  \nfreq                   NaN            9136  \nmean         206855.816909             NaN  \nstd          115395.615874             NaN  \nmin           14999.000000             NaN  \n25%          119600.000000             NaN  \n50%          179700.000000             NaN  \n75%          264725.000000             NaN  \nmax          500001.000000             NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n      <th>median_house_value</th>\n      <th>ocean_proximity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20433.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>&lt;1H OCEAN</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>9136</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-119.569704</td>\n      <td>35.631861</td>\n      <td>28.639486</td>\n      <td>2635.763081</td>\n      <td>537.870553</td>\n      <td>1425.476744</td>\n      <td>499.539680</td>\n      <td>3.870671</td>\n      <td>206855.816909</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>2.003532</td>\n      <td>2.135952</td>\n      <td>12.585558</td>\n      <td>2181.615252</td>\n      <td>421.385070</td>\n      <td>1132.462122</td>\n      <td>382.329753</td>\n      <td>1.899822</td>\n      <td>115395.615874</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-124.350000</td>\n      <td>32.540000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n      <td>0.499900</td>\n      <td>14999.000000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-121.800000</td>\n      <td>33.930000</td>\n      <td>18.000000</td>\n      <td>1447.750000</td>\n      <td>296.000000</td>\n      <td>787.000000</td>\n      <td>280.000000</td>\n      <td>2.563400</td>\n      <td>119600.000000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-118.490000</td>\n      <td>34.260000</td>\n      <td>29.000000</td>\n      <td>2127.000000</td>\n      <td>435.000000</td>\n      <td>1166.000000</td>\n      <td>409.000000</td>\n      <td>3.534800</td>\n      <td>179700.000000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>-118.010000</td>\n      <td>37.710000</td>\n      <td>37.000000</td>\n      <td>3148.000000</td>\n      <td>647.000000</td>\n      <td>1725.000000</td>\n      <td>605.000000</td>\n      <td>4.743250</td>\n      <td>264725.000000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>-114.310000</td>\n      <td>41.950000</td>\n      <td>52.000000</td>\n      <td>39320.000000</td>\n      <td>6445.000000</td>\n      <td>35682.000000</td>\n      <td>6082.000000</td>\n      <td>15.000100</td>\n      <td>500001.000000</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe(include='all')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As it was analyzed in the previous exercise of Exploratory Data Analysis ([notebook](ExploratoryDataAnalysis.ipynb), [report](/reports/ExploratoryDataAnalysis.pdf)) the actions taken for Data Cleaning and Feature Engineering are:\n",
    "* Target normalization\n",
    "* Handling missing values\n",
    "* Handling outliers\n",
    "* Encoding categorical variables\n",
    "* Scaling continuous variables\n",
    "\n",
    "And these actions are encapsulated in the method `prepare_data()` from the `utils/data_transformations.py`, but first\n",
    "the data must be split to create the train and test set to avoid overfitting and inaccurate evaluation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data.drop(columns=[TARGET], axis=1), data[TARGET], random_state=42, test_size=0.3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "' Shape x_train (14448, 9) - y_train (14448,)'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\" Shape x_train {x_train.shape} - y_train {y_train.shape}\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "' Shape x_test (6192, 9) - y_test (6192,)'"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\" Shape x_test {x_test.shape} - y_test {y_test.shape}\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Objectives\n",
    "\n",
    "This exercise focuses in the predictions of the models, so this approach compares $y_p$ with $y$ by **performance metrics**,\n",
    "which measure the quality of the model's predictions (closeness between $y_p$ and $y$).\n",
    "\n",
    "As this approach doesn't focus on interpretability there is a greater risk of having a Black-box model, so it's recommended\n",
    "also to explore an approach based in interpretation to have both."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO\n",
    "\n",
    "- Summary of training at least three linear regression models which should be variations that cover using a simple linear regression as a baseline, adding polynomial effects, and using a regularization regression. Preferably, all use the same training and test splits, or the same cross-validation method.\n",
    "- A paragraph explaining which of your regressions you recommend as a final model that best fits your needs in terms of accuracy and explainability.\n",
    "- Summary Key Findings and Insights, which walks your reader through the main drivers of your model and insights from your data derived from your linear regression model.\n",
    "- Suggestions for next steps in analyzing this data, which may include suggesting revisiting this model adding specific data features to achieve a better explanation or a better prediction."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Linear Regression Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return numpy.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "def add_metrics(metrics_report: dict, model_name: str, y_true, y_pred) -> dict:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        metrics_report:\n",
    "        model_name:\n",
    "        y_true:\n",
    "        y_pred:\n",
    "\n",
    "    Returns:\n",
    "        dict: per model\n",
    "                - MSE: penalizes big errors\n",
    "                - RMSE: standarize unit errors\n",
    "                - R2: proportion of variance (0,1) - The bigger better\n",
    "                - MAE: average of erros\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(y_true=y_true, y_pred=y_pred)\n",
    "    rmse = numpy.sqrt(mse)\n",
    "    r2 = r2_score(y_true=y_true, y_pred=y_pred)\n",
    "    mae = mean_absolute_error(y_true=y_true, y_pred=y_pred)\n",
    "\n",
    "    metrics_report[model_name] = {\n",
    "        'MSE': round(mse, 4),\n",
    "        'RMSE': round(rmse, 4),\n",
    "        'R2': round(r2, 4),\n",
    "        'MAE': round(mae, 4)\n",
    "    }\n",
    "    return metrics_report"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Preparing the data\n",
    "\n",
    "As mentioned in the Exploratory Data Analysis indicates that there is collinearity between some variables."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    longitude  latitude  housing_median_age  total_rooms  \\\n",
      "longitude            1.000000 -0.923408           -0.101083     0.037158   \n",
      "latitude            -0.923408  1.000000            0.003461    -0.028768   \n",
      "housing_median_age  -0.101083  0.003461            1.000000    -0.362713   \n",
      "total_rooms          0.037158 -0.028768           -0.362713     1.000000   \n",
      "total_bedrooms       0.061797 -0.059700           -0.321328     0.929527   \n",
      "population           0.092163 -0.101665           -0.291589     0.855384   \n",
      "households           0.047659 -0.063487           -0.302516     0.920133   \n",
      "median_income       -0.019019 -0.075892           -0.117506     0.198362   \n",
      "\n",
      "                    total_bedrooms  population  households  median_income  \n",
      "longitude                 0.061797    0.092163    0.047659      -0.019019  \n",
      "latitude                 -0.059700   -0.101665   -0.063487      -0.075892  \n",
      "housing_median_age       -0.321328   -0.291589   -0.302516      -0.117506  \n",
      "total_rooms               0.929527    0.855384    0.920133       0.198362  \n",
      "total_bedrooms            1.000000    0.876119    0.980570      -0.010193  \n",
      "population                0.876119    1.000000    0.904678       0.003661  \n",
      "households                0.980570    0.904678    1.000000       0.011628  \n",
      "median_income            -0.010193    0.003661    0.011628       1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Compute correlation matrix\n",
    "corr_matrix = x_train.corr(numeric_only=True)\n",
    "\n",
    "print(corr_matrix)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longitude             614.104270\n",
      "latitude              548.596876\n",
      "housing_median_age      7.259158\n",
      "total_rooms            30.774710\n",
      "total_bedrooms         95.973312\n",
      "population             15.813275\n",
      "households             94.392021\n",
      "median_income           8.260040\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "numerical_X_train = x_train.select_dtypes(include='number')\n",
    "\n",
    "# Compute VIF scores\n",
    "vif_scores = pandas.Series([variance_inflation_factor(numerical_X_train.values, i) for i in range(numerical_X_train.shape[1])], index=numerical_X_train.columns)\n",
    "\n",
    "# Print VIF scores\n",
    "print(vif_scores)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "                    longitude  latitude  housing_median_age  total_rooms  \\\nlongitude                 1.0       NaN                 NaN          NaN   \nlatitude                  NaN       1.0                 NaN          NaN   \nhousing_median_age        NaN       NaN                 1.0          NaN   \ntotal_rooms               NaN       NaN                 NaN     1.000000   \ntotal_bedrooms            NaN       NaN                 NaN     0.929527   \npopulation                NaN       NaN                 NaN     0.855384   \nhouseholds                NaN       NaN                 NaN     0.920133   \nmedian_income             NaN       NaN                 NaN          NaN   \n\n                    total_bedrooms  population  households  median_income  \nlongitude                      NaN         NaN         NaN            NaN  \nlatitude                       NaN         NaN         NaN            NaN  \nhousing_median_age             NaN         NaN         NaN            NaN  \ntotal_rooms               0.929527    0.855384    0.920133            NaN  \ntotal_bedrooms            1.000000    0.876119    0.980570            NaN  \npopulation                0.876119    1.000000    0.904678            NaN  \nhouseholds                0.980570    0.904678    1.000000            NaN  \nmedian_income                  NaN         NaN         NaN            1.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>longitude</th>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>latitude</th>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>housing_median_age</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>total_rooms</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.000000</td>\n      <td>0.929527</td>\n      <td>0.855384</td>\n      <td>0.920133</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>total_bedrooms</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.929527</td>\n      <td>1.000000</td>\n      <td>0.876119</td>\n      <td>0.980570</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>population</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.855384</td>\n      <td>0.876119</td>\n      <td>1.000000</td>\n      <td>0.904678</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>households</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.920133</td>\n      <td>0.980570</td>\n      <td>0.904678</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>median_income</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix[corr_matrix > 0.7]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on the results from the VIF score and correlation matrix, it can be observed that `total_rooms` and `total_bedrooms` have high values of VIF, indicating that these two variables are highly correlated. This high correlation is not surprising, as `total_rooms` may affect the number of `total_bedrooms`. However, including both features in the model can lead to issues with multicollinearity, as it becomes difficult to distinguish the individual effect of each variable on the target variable. Therefore, it may be necessary to address multicollinearity in the model, either by removing one of the highly correlated features or by using techniques such as ridge regression or principal component analysis.\n",
    "\n",
    "The same with `population` and `households`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "vars_to_remove = ['total_rooms', 'population', 'households']\n",
    "x_train = x_train.drop(columns=vars_to_remove, axis=1)\n",
    "x_test = x_test.drop(columns=vars_to_remove, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "importlib.reload(sys.modules['src.utils.data_transformations'])\n",
    "from src.utils.data_transformations import preprocess_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DataFrame constructor not properly called!",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m vars_with_outliers \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmedian_income\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtotal_bedrooms\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      2\u001B[0m                       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhousing_median_age\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m      4\u001B[0m x_train, y_train, preprocessor, lmbda \u001B[38;5;241m=\u001B[39m preprocess_data(X\u001B[38;5;241m=\u001B[39mx_train, y\u001B[38;5;241m=\u001B[39my_train, variables_with_outliers\u001B[38;5;241m=\u001B[39mvars_with_outliers, normalize_target\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m----> 5\u001B[0m x_test, y_test, _, _ \u001B[38;5;241m=\u001B[39m \u001B[43mpreprocess_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocessor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreprocessor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvariables_with_outliers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvars_with_outliers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m                                       \u001B[49m\u001B[43mnormalize_target\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnormalization_lmbda\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlmbda\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Work/Projects/ML_Projects/CaliforniaHousingCensusCase/src/utils/data_transformations.py:165\u001B[0m, in \u001B[0;36mpreprocess_data\u001B[0;34m(X, y, normalize_target, normalization_lmbda, variables_with_outliers, preprocessor, verbose)\u001B[0m\n\u001B[1;32m    162\u001B[0m lamb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m normalize_target:\n\u001B[1;32m    164\u001B[0m     \u001B[38;5;66;03m# msg += f\"{'':>30}{'Old':10}: {y.skew():.4f}. \\n\"\u001B[39;00m\n\u001B[0;32m--> 165\u001B[0m     y, lamb \u001B[38;5;241m=\u001B[39m \u001B[43mnormalize_column\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlmbda\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnormalization_lmbda\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    166\u001B[0m     \u001B[38;5;66;03m# msg += f\"{'':>30}{'Current':10}: {y.skew():.4f}\\n\"\u001B[39;00m\n\u001B[1;32m    168\u001B[0m msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m>30\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAfter\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m10\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mX\u001B[38;5;241m.\u001B[39mtotal_bedrooms\u001B[38;5;241m.\u001B[39misna()\u001B[38;5;241m.\u001B[39msum()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \\\n\u001B[1;32m    169\u001B[0m        \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m-> Handling outliers.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m~/Work/Projects/ML_Projects/CaliforniaHousingCensusCase/src/utils/data_transformations.py:82\u001B[0m, in \u001B[0;36mnormalize_column\u001B[0;34m(data, column, lmbda)\u001B[0m\n\u001B[1;32m     80\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     81\u001B[0m     bc_result \u001B[38;5;241m=\u001B[39m boxcox(data, lmbda\u001B[38;5;241m=\u001B[39mlmbda)\n\u001B[0;32m---> 82\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[43mDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbc_result\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     84\u001B[0m lamb \u001B[38;5;241m=\u001B[39m bc_result[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data, lamb\n",
      "File \u001B[0;32m~/Work/Projects/ML_Projects/CaliforniaHousingCensusCase/venv/lib/python3.10/site-packages/pandas/core/frame.py:781\u001B[0m, in \u001B[0;36mDataFrame.__init__\u001B[0;34m(self, data, index, columns, dtype, copy)\u001B[0m\n\u001B[1;32m    778\u001B[0m \u001B[38;5;66;03m# For data is scalar\u001B[39;00m\n\u001B[1;32m    779\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    780\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m index \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m columns \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 781\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame constructor not properly called!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    783\u001B[0m     index \u001B[38;5;241m=\u001B[39m ensure_index(index)\n\u001B[1;32m    784\u001B[0m     columns \u001B[38;5;241m=\u001B[39m ensure_index(columns)\n",
      "\u001B[0;31mValueError\u001B[0m: DataFrame constructor not properly called!"
     ]
    }
   ],
   "source": [
    "vars_with_outliers = [\"median_income\", \"total_bedrooms\",\n",
    "                      \"housing_median_age\"]\n",
    "\n",
    "x_train, y_train, preprocessor, lmbda = preprocess_data(X=x_train, y=y_train, variables_with_outliers=vars_with_outliers, normalize_target=True)\n",
    "x_test, y_test, _, _ = preprocess_data(X=x_test, y=y_test, preprocessor=preprocessor, variables_with_outliers=vars_with_outliers,\n",
    "                                       normalize_target=True, normalization_lmbda=lmbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preprocessor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Simple Linear Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr = GridSearchCV(estimator=LinearRegression(n_jobs=-1),\n",
    "                  n_jobs=-1,\n",
    "                  verbose=1,\n",
    "                  param_grid={},\n",
    "                  cv=5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr.fit(X=x_train, y=y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics = add_metrics(metrics_report=metrics,\n",
    "                      model_name='LR_Simple',\n",
    "                      y_true=y_test,\n",
    "                      y_pred=lr.predict(x_test))\n",
    "\n",
    "print(f\"{metrics['LR_Simple']}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Linear Regression with Polynomial Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_polyn_pipe = Pipeline(steps=[\n",
    "    ('polynomail', PolynomialFeatures(degree=3, include_bias=False)),\n",
    "    ('lr', LinearRegression(n_jobs=-1))\n",
    "])\n",
    "\n",
    "lr_polynomial = GridSearchCV(\n",
    "    estimator=model_polyn_pipe,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    param_grid={},\n",
    "    cv=5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_polynomial.fit(X=x_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics = add_metrics(metrics_report=metrics,\n",
    "                      model_name='LR_PolynEffects',\n",
    "                      y_true=y_test,\n",
    "                      y_pred=lr_polynomial.predict(x_test))\n",
    "\n",
    "print(f\"{metrics['LR_PolynEffects']}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Regression with Regularization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "l1_ratios = numpy.linspace(0.1, 0.9, 9)\n",
    "alphas = numpy.array([1e-5, 5e-5, 0.0001, 0.0005])\n",
    "\n",
    "lr_regularization = ElasticNetCV(\n",
    "    alphas=alphas,\n",
    "    l1_ratio=l1_ratios,\n",
    "    n_jobs=-1,\n",
    "    max_iter=int(1e4),\n",
    "    cv=5\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr_regularization.fit(X=x_train, y=y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics = add_metrics(metrics_report=metrics,\n",
    "                      model_name='Regularization',\n",
    "                      y_true=y_test,\n",
    "                      y_pred=lr_regularization.predict(x_test))\n",
    "\n",
    "print(f\"{metrics['Regularization']}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Processing data:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"LR_Simple\": {\n",
    "        \"MSE\": 6784237334.8747,\n",
    "        \"RMSE\": 82366.4819,\n",
    "        \"R2\": 0.4831,\n",
    "        \"MAE\": 59813.3505\n",
    "    },\n",
    "    \"LR_PolynEffects\": {\n",
    "        \"MSE\": 5496039866.1283,\n",
    "        \"RMSE\": 74135.2808,\n",
    "        \"R2\": 0.5813,\n",
    "        \"MAE\": 52709.0184\n",
    "    },\n",
    "    \"Regularization\": {\n",
    "        \"MSE\": 6784518279.5301,\n",
    "        \"RMSE\": 82368.1873,\n",
    "        \"R2\": 0.4831,\n",
    "        \"MAE\": 59803.6537\n",
    "    }\n",
    "}\n",
    "\n",
    "# without splitting first\n",
    "\n",
    "\n",
    "    \"LR_Simple\": {\n",
    "        \"MSE\": 6759249893.0372,\n",
    "        \"RMSE\": 82214.6574,\n",
    "        \"R2\": 0.485,\n",
    "        \"MAE\": 59836.6745\n",
    "    },\n",
    "    \"LR_PolynEffects\": {\n",
    "        \"MSE\": 7.773524881389625e+23,\n",
    "        \"RMSE\": 881675954157.1736,\n",
    "        \"R2\": -59225046420386.34,\n",
    "        \"MAE\": 11204573367.0155\n",
    "    },\n",
    "    \"Regularization\": {\n",
    "        \"MSE\": 6759466449.1363,\n",
    "        \"RMSE\": 82215.9744,\n",
    "        \"R2\": 0.485,\n",
    "        \"MAE\": 59830.8617\n",
    "    }\n",
    "}\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "print(json.dumps(metrics, indent=4))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_test.mean(), y_test.median(), y_test.min(), y_test.max()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Insights and key findings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Next Steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>Author: Jose Pena</p>\n",
    "<p>Github: <a href=\"https://github.com/JoseJuan98\">JoseJuan98</a></p>"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Change Log\n",
    "| Date (YYYY-MM-DD) | Version | Changed By | Change Description   |\n",
    "|-------------------| ------- | ---------- | -------------------- |\n",
    "| 2022-11-27        | 1.0     | Jose       | First version        |\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
